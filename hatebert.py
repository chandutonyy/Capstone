# -*- coding: utf-8 -*-
"""HateBERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pGzttet8dG9fPHuHujygpBVvhOdyJFWI
"""

"""
Actual model is trained and tested in the above colab notebook whereas parts of code from the notebook
parts of necessary code snippets and the trained model is taken from the above notebook to get here
and get this program running in the site.
"""
import pandas as pd
import torch
from transformers import BertForSequenceClassification, BertTokenizer
# from torch.utils.data import Dataset, DataLoader
# from sklearn.model_selection import train_test_split
from tabulate import tabulate
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from google.cloud import storage
print(
    "\n\n-------------------------------------------Importing libraries---------------------------------------------------")





def download_blob(bucket_name, source_blob_name, destination_file_name):
    """Downloads a blob from the bucket."""
    # bucket_name = "your-bucket-name"
    # source_blob_name = "storage-object-name"
    # destination_file_name = "local/path/to/file"

    storage_client = storage.Client()

    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(source_blob_name)
    blob.download_to_filename(destination_file_name)

    print(
        "Blob {} downloaded to {}.".format(
            source_blob_name, destination_file_name
        )
    )

print(
    "\n\n-------------------------------------------Downloading (Dataset)/model from GCS---------------------------------------------------")

# # Download the train.csv file
# download_blob('safedialogue', 'SafeDialogue/', 'train.csv')

# Download the model.bin file
download_blob('safedialogue', 'SafeDialogue/', 'model.bin')



# class HateSpeechDataset(Dataset):

#     def __init__(self, dataframe, tokenizer, max_len):
#         self.tokenizer = tokenizer
#         self.data = dataframe
#         self.comment_text = dataframe.comment_text
#         self.targets = self.data[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values
#         self.max_len = max_len

#     def __len__(self):
#         return len(self.comment_text)

#     def __getitem__(self, index):
#         comment_text = str(self.comment_text.iloc[index])
#         comment_text = " ".join(comment_text.split())

#         inputs = self.tokenizer.encode_plus(
#             comment_text,
#             None,
#             add_special_tokens=True,
#             max_length=self.max_len,
#             padding='max_length',
#             return_token_type_ids=True
#         )
#         ids = inputs['input_ids']
#         mask = inputs['attention_mask']

#         return {
#             'ids': torch.tensor(ids, dtype=torch.long),
#             'mask': torch.tensor(mask, dtype=torch.long),
#             'targets': torch.tensor(self.targets[index], dtype=torch.float)
#         }

# print("\n\n-------------------------------------------Dataset function running---------------------------------------------------")
# Parameters
MAX_LEN = 200
TRAIN_BATCH_SIZE = 8
VALID_BATCH_SIZE = 4
EPOCHS = 1
LEARNING_RATE = 1e-05
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# # Load the dataset
# train_dataset = pd.read_csv('train.csv')
# print("\n\n---------------------------------------------Dataset Loaded-------------------------------------------------")

# # Split into training and validation sets
# train_data, val_data = train_test_split(train_dataset, test_size=0.1)

# # Create the HateSpeechDataset
# training_set = HateSpeechDataset(train_data, tokenizer, MAX_LEN)
# val_set = HateSpeechDataset(val_data, tokenizer, MAX_LEN)

# train_data.reset_index(drop=True, inplace=True)
# val_data.reset_index(drop=True, inplace=True)

# train_params = {'batch_size': TRAIN_BATCH_SIZE,
#                 'shuffle': True,
#                 'num_workers': 0
#                 }

# val_params = {'batch_size': VALID_BATCH_SIZE,
#               'shuffle': False,
#               'num_workers': 0
#               }

# training_loader = DataLoader(training_set, **train_params)
# val_loader = DataLoader(val_set, **val_params)
# print("\n\n---------------------------------------------Data Loader Successful-------------------------------------------------")

model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6)
# # model.to(device)
print(
    "\n\n-------------------------------------------Model Downloaded---------------------------------------------------")

loss_function = torch.nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)
"""
# # Dont run the below cell often
# 
# model.train()
# for epoch in range(EPOCHS):
#     for _, data in enumerate(training_loader, 0):
#         ids = data['ids'].to(device, dtype=torch.long)
#         mask = data['mask'].to(device, dtype=torch.long)
#         targets = data['targets'].to(device, dtype=torch.float)
# 
#         outputs = model(ids, mask).logits
#         optimizer.zero_grad()
#         loss = loss_function(outputs, targets)
#         if _%500==0:
#             print(f'Epoch: {epoch}, Loss:  {loss.item()}')
# 
#         optimizer.zero_grad()
#         loss.backward()
#         optimizer.step()
# 
# # Dont run the above cell
"""
"""Model is Complete, saving the model to call it next time and for presentation to not run the model over and over again since the training loop takes almost 100 minutes to complete

"""

model_save_path = "model.bin"


def load_model():
    model_save_path = "model.bin"
    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6)
    model.load_state_dict(torch.load(model_save_path, map_location=torch.device('cpu')))
    model.eval()
    return model


def load_tokenizer():
    return BertTokenizer.from_pretrained('bert-base-uncased')


load_model()
print(
    "\n\n----------------------------------------------------------Saved Model retrieved--------------------------------------------------")


def classify_toxicity(sentence, model, tokenizer):
    inputs = tokenizer.encode_plus(
        sentence,
        None,
        add_special_tokens=True,
        max_length=200,
        padding='max_length',
        return_token_type_ids=True
    )
    ids = torch.tensor([inputs['input_ids']], dtype=torch.long)
    mask = torch.tensor([inputs['attention_mask']], dtype=torch.long)
    outputs = model(ids, mask).logits
    outputs = torch.sigmoid(outputs).detach().cpu().numpy()
    outputs = outputs[0]  # get the probabilities

    class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']
    # Associate class names with their probabilities
    result = dict(zip(class_names, outputs))

    # Exclude "toxic" from the class comparison
    result_no_toxic = result.copy()
    del result_no_toxic["toxic"]

    # Sort the classes based on their probabilities
    sorted_classes = sorted(result_no_toxic.items(), key=lambda item: item[1], reverse=True)

    # Check if the highest probability is below the defined threshold
    if sorted_classes[0][1] < 0.4:
        result_text = "This sentence seems perfectly alright for public viewing."
    else:
        # Provide an interpretation based on the highest probability class
        result_text = f"This sentence is predominantly {sorted_classes[0][0]} with a probability of {sorted_classes[0][1]:.2f}."
        # Also provide information on the next most probable class, if its probability is significant
        if sorted_classes[1][1] > 0.2:
            result_text += f"\nIt also exhibits characteristics of {sorted_classes[1][0]} with a probability of {sorted_classes[1][1]:.2f}."
    return result, result_text, result_no_toxic


def plot_toxicity(result_no_toxic):
    plt.figure(figsize=[14, 14])
    plt.pie(result_no_toxic.values(), labels=result_no_toxic.keys(), autopct='%1.1f%%',
            textprops={'font-size': 14})  # Increase the font-size
    plt.title("Toxicity Classification", fontsize=16)  # Increase the title font-size
    plt.legend(result_no_toxic.keys(), loc='best')
    # plt.barh(list(result_no_toxic.keys()), list(result_no_toxic.values()))

    plt.title("Toxicity Classification")

    plot_file = "static/plot.png"
    plt.savefig(plot_file)

    return plot_file


def user_input_fn(sentence, model, tokenizer):
    prediction, prediction_text, result_no_toxic = classify_toxicity(sentence, model, tokenizer)
    plot_file = plot_toxicity(result_no_toxic)
    return prediction, prediction_text, plot_file


print("\n\n Open the url and start testing............................\n\n")

user_input_fn("I am gonna fucking kill you ", model, tokenizer)
